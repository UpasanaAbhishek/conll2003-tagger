{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    file = open(path)\n",
    "    file.seek(0)\n",
    "    file_content = file.read()\n",
    "    file.close()\n",
    "    return file_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to convert read files to sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sent(file_content):\n",
    "    docs = file_content.split(\"-DOCSTART- -X- -X- O\")\n",
    "    sentence_list = []\n",
    "    temp_list = []\n",
    "    for doc in docs:\n",
    "        doc = doc.strip()\n",
    "        sentence = doc.split(\"\\n\")\n",
    "        for sub_string in sentence:\n",
    "            temp_list.append(sub_string)\n",
    "            if sub_string == \"\":    \n",
    "                sentence_list.append(temp_list)\n",
    "                temp_list = []\n",
    "    all_sentences = []\n",
    "    for i in range(len(sentence_list)):\n",
    "        if sentence_list[i] == [\"\"]:\n",
    "            continue\n",
    "        sent= []\n",
    "        for j in range(len(sentence_list[i])):\n",
    "            a = sentence_list[i][j].split(\" \")\n",
    "            if len(a)==4:\n",
    "                sent.append(a)\n",
    "        all_sentences.append(sent)\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract all words and tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_and_tags(all_sentences):\n",
    "    (words, chunk, pos, net, words_list, \n",
    "        chunk_list, pos_list, net_list) = [],[],[],[],[],[],[],[]\n",
    "    for sentence in all_sentences:\n",
    "        for word in sentence:\n",
    "            words.append(word[0])\n",
    "            chunk.append(word[1])\n",
    "            pos.append(word[2])\n",
    "            net.append(word[3])\n",
    "        words_list.append(words)\n",
    "        chunk_list.append(chunk)\n",
    "        pos_list.append(pos)\n",
    "        net_list.append(net)\n",
    "        words, chunk, pos, net = [],[],[],[]\n",
    "    return words_list, chunk_list, pos_list, net_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to flatten lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(input_list):\n",
    "    flat_list = []\n",
    "    for sublist in input_list:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate tag dictictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(flist):\n",
    "    tag_set = set(flist)\n",
    "    tag_dict = {t: i + 1 for i, t in enumerate(list(tag_set))}\n",
    "    tag_dict['-PAD-'] = 0  # The special value used to padding\n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to transform sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_transformation(sentence_list, word_dict):\n",
    "    new_sentences = []\n",
    "    for sent in sentence_list:\n",
    "        num_words = []\n",
    "        for word in sent:\n",
    "            if word in word_dict.keys():\n",
    "                num_words.append(word_dict[word])\n",
    "            else:\n",
    "                num_words.append(word_dict[\"-OOV-\"])\n",
    "        new_sentences.append(num_words)    \n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(words_list, chunk_list, pos_list, net_list, max_length, dict_val = None):\n",
    "    (fwords_list, fchunk_list, fpos_list, fnet_list) = (\n",
    "        flatten_list(words_list), flatten_list(chunk_list), \n",
    "        flatten_list(pos_list), flatten_list(net_list))\n",
    "    words = set(fwords_list)\n",
    "    word_dict = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "    word_dict['-PAD-'] = 0  # The special value used for padding\n",
    "    word_dict['-OOV-'] = 1\n",
    "    \n",
    "    if dict_val == None:    \n",
    "        x = sentence_transformation(words_list, word_dict)\n",
    "        y_chunk = sentence_transformation(chunk_list, generate_dict(fchunk_list))\n",
    "        y_pos = sentence_transformation(pos_list, generate_dict(fpos_list))\n",
    "        y_net = sentence_transformation(net_list, generate_dict(fnet_list))\n",
    "    \n",
    "        return {\n",
    "            \"x\": pad_sequences(x, maxlen=max_length, padding='post'),\n",
    "            \"y_chunk\": pad_sequences(y_chunk, maxlen=max_length, padding='post'),\n",
    "            \"y_pos\"  : pad_sequences(y_pos, maxlen=max_length, padding='post'), \n",
    "            \"y_net\"  : pad_sequences(y_net, maxlen=max_length, padding='post'),\n",
    "            \"tag_dict\" : {\n",
    "                \"word_dict\" : word_dict,\n",
    "                \"chunk_dict\" : generate_dict(fchunk_list),\n",
    "                \"pos_dict\" : generate_dict(fpos_list),\n",
    "                \"net_dict\" : generate_dict(fnet_list)\n",
    "            }\n",
    "        }\n",
    "    else: \n",
    "        x = sentence_transformation(words_list, dict_val[\"word_dict\"])\n",
    "        y_chunk = sentence_transformation(chunk_list, dict_val[\"chunk_dict\"])\n",
    "        y_pos = sentence_transformation(pos_list, dict_val[\"pos_dict\"])\n",
    "        y_net = sentence_transformation(net_list, dict_val[\"net_dict\"])\n",
    "    \n",
    "        return {\n",
    "            \"x\": pad_sequences(x, maxlen=max_length, padding='post'),\n",
    "            \"y_chunk\": pad_sequences(y_chunk, maxlen=max_length, padding='post'),\n",
    "            \"y_pos\"  : pad_sequences(y_pos, maxlen=max_length, padding='post'), \n",
    "            \"y_net\"  : pad_sequences(y_net, maxlen=max_length, padding='post')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path, max_length=50, dict_val=None):\n",
    "    file_content = read_file(file_path)\n",
    "    file_sentences = convert_to_sent(file_content)\n",
    "    (words_list, chunk_list, pos_list, \n",
    "        net_list) = extract_word_and_tags(file_sentences)\n",
    "    processed_data = preprocess_data(words_list, chunk_list, pos_list, net_list, max_length, dict_val)    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model, x_data, y_data, inverse_dict):\n",
    "    pred = model.predict_classes(x_data)\n",
    "    pred_names = sentence_transformation(pred, inverse_dict)\n",
    "    true_names = sentence_transformation(y_data, inverse_dict)\n",
    "    pred_names_flat = flatten_list(pred_names)\n",
    "    true_names_flat = flatten_list(true_names)\n",
    "    pred_ind = flatten_list(pred)\n",
    "    true_ind = flatten_list(y_data)\n",
    "    x_words = sentence_transformation(x_data, inversed_word_dict)\n",
    "    words_flat = flatten_list(x_words)\n",
    "    return pd.DataFrame({\"words\":words_flat, \"true_names\":true_names_flat,\n",
    "                         \"pred_names\":pred_names_flat, \n",
    "                   \"true_ind\":true_ind, \"pred_ind\": pred_ind})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_diag(df):\n",
    "    f1 = f1_score(df[\"true_names\"], df[\"pred_names\"], average=\"weighted\")\n",
    "    print('F1 score: %f' % f1)\n",
    "    kappa = cohen_kappa_score(df[\"true_names\"], df[\"pred_names\"])\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    accuracy = accuracy_score(df[\"true_names\"], df[\"pred_names\"])\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(df[\"true_names\"], df[\"pred_names\"], average=\"weighted\")\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\"\"\n",
    "    recall = recall_score(df[\"true_names\"], df[\"pred_names\"], average=\"weighted\")\n",
    "    print('Recall: %f' % recall)\n",
    "    #print(classification_report(df[\"true_names\"], df[\"pred_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding(df):\n",
    "    df=df[df[\"words\"]!=\"-PAD-\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = r\"eng.train\"\n",
    "validation = r\"eng.testa\"\n",
    "test = r\"eng.testb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "prepared_data_train = prepare_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['word_dict', 'chunk_dict', 'pos_dict', 'net_dict'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_data_train[\"tag_dict\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_validation = prepare_data(validation, max_length=50, dict_val=prepared_data_train[\"tag_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_test = prepare_data(test, max_length=50, dict_val=prepared_data_train[\"tag_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversed_word_dict = dict([(value, key) for key, value in prepared_data_train[\"tag_dict\"][\"word_dict\"].items()])\n",
    "inversed_pos_dict = dict([(value, key) for key, value in prepared_data_train[\"tag_dict\"][\"pos_dict\"].items()])\n",
    "inversed_chunk_dict = dict([(value, key) for key, value in prepared_data_train[\"tag_dict\"][\"chunk_dict\"].items()])\n",
    "inversed_net_dict = dict([(value, key) for key, value in prepared_data_train[\"tag_dict\"][\"net_dict\"].items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amandeepsingh\\Miniconda3\\envs\\learning_ml\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 128)           3024000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 512)           788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 18)            9234      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50, 18)            0         \n",
      "=================================================================\n",
      "Total params: 3,821,714\n",
      "Trainable params: 3,821,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pos_model = Sequential()\n",
    "pos_model.add(InputLayer(input_shape=(max_len, )))\n",
    "pos_model.add(Embedding(len(prepared_data_train[\"tag_dict\"][\"word_dict\"]), 128))\n",
    "pos_model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "pos_model.add(TimeDistributed(Dense(len(prepared_data_train[\"tag_dict\"][\"pos_dict\"]))))\n",
    "pos_model.add(Activation('softmax'))\n",
    "pos_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(0.001),\n",
    "             metrics=['accuracy'])\n",
    "pos_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amandeepsingh\\Miniconda3\\envs\\learning_ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\amandeepsingh\\Miniconda3\\envs\\learning_ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 13095 samples, validate on 3034 samples\n",
      "Epoch 1/2\n",
      "13095/13095 [==============================] - 20s 2ms/step - loss: 0.6082 - accuracy: 0.8368 - val_loss: 0.4070 - val_accuracy: 0.8686\n",
      "Epoch 2/2\n",
      "13095/13095 [==============================] - 20s 1ms/step - loss: 0.2281 - accuracy: 0.9340 - val_loss: 0.1408 - val_accuracy: 0.9653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13d442e7588>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_model.fit(\n",
    "    prepared_data_train[\"x\"],\n",
    "    to_categorical(prepared_data_train[\"y_pos\"], len(prepared_data_train[\"tag_dict\"][\"pos_dict\"])),\n",
    "    batch_size=128, epochs=2,\n",
    "    validation_data = (\n",
    "        prepared_data_validation[\"x\"],\n",
    "        to_categorical(prepared_data_validation[\"y_pos\"], len(prepared_data_train[\"tag_dict\"][\"pos_dict\"]))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diag_pos = remove_padding(predictions(pos_model,\n",
    "           prepared_data_train[\"x\"],\n",
    "           prepared_data_train[\"y_pos\"],\n",
    "           inverse_dict=inversed_pos_dict))\n",
    "validation_diag_pos = remove_padding(predictions(pos_model,\n",
    "           prepared_data_validation[\"x\"],\n",
    "           prepared_data_validation[\"y_pos\"],\n",
    "           inverse_dict=inversed_pos_dict))\n",
    "test_diag_pos= remove_padding(predictions(pos_model,\n",
    "           prepared_data_test[\"x\"],\n",
    "           prepared_data_test[\"y_pos\"],\n",
    "           inverse_dict=inversed_pos_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "F1 score: 0.881430\n",
      "Cohens kappa: 0.836863\n",
      "Accuracy: 0.905222\n",
      "Precision: 0.859103\n",
      "Recall: 0.905222\n",
      "Validation metrics:\n",
      "F1 score: 0.871938\n",
      "Cohens kappa: 0.823751\n",
      "Accuracy: 0.896644\n",
      "Precision: 0.849006\n",
      "Recall: 0.896644\n",
      "Test metrics:\n",
      "F1 score: 0.876114\n",
      "Cohens kappa: 0.821555\n",
      "Accuracy: 0.899900\n",
      "Precision: 0.853933\n",
      "Recall: 0.899900\n"
     ]
    }
   ],
   "source": [
    "print(\"Train metrics:\")\n",
    "print_diag(train_diag_pos)\n",
    "print(\"Validation metrics:\")\n",
    "print_diag(validation_diag_pos)\n",
    "print(\"Test metrics:\")\n",
    "print_diag(test_diag_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       -PAD-       0.00      0.00      0.00         0\n",
      "      B-ADJP       0.00      0.00      0.00         2\n",
      "      B-ADVP       0.00      0.00      0.00        22\n",
      "        B-NP       0.00      0.00      0.00      3771\n",
      "        B-PP       0.00      0.00      0.00       253\n",
      "      B-SBAR       0.00      0.00      0.00         8\n",
      "        B-VP       0.00      0.00      0.00       163\n",
      "      I-ADJP       0.00      0.00      0.00      1370\n",
      "      I-ADVP       0.00      0.00      0.00      2747\n",
      "     I-CONJP       0.00      0.00      0.00        70\n",
      "      I-INTJ       0.00      0.00      0.00        60\n",
      "       I-LST       0.00      0.00      0.00        36\n",
      "        I-NP       0.91      0.98      0.95    119975\n",
      "        I-PP       0.93      0.93      0.93     18651\n",
      "       I-PRT       0.00      0.00      0.00       527\n",
      "      I-SBAR       0.00      0.00      0.00      1275\n",
      "        I-VP       0.82      0.83      0.83     26653\n",
      "           O       0.94      0.97      0.95     27587\n",
      "\n",
      "    accuracy                           0.91    203170\n",
      "   macro avg       0.20      0.21      0.20    203170\n",
      "weighted avg       0.86      0.91      0.88    203170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_diag_pos[\"true_names\"], train_diag_pos[\"pred_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       -PAD-       0.00      0.00      0.00         0\n",
      "      B-ADVP       0.00      0.00      0.00         5\n",
      "        B-NP       0.00      0.00      0.00       968\n",
      "        B-PP       0.00      0.00      0.00        53\n",
      "      B-SBAR       0.00      0.00      0.00         5\n",
      "        B-VP       0.00      0.00      0.00        38\n",
      "      I-ADJP       0.00      0.00      0.00       357\n",
      "      I-ADVP       0.00      0.00      0.00       680\n",
      "     I-CONJP       0.00      0.00      0.00        23\n",
      "      I-INTJ       0.00      0.00      0.00        31\n",
      "       I-LST       0.00      0.00      0.00         3\n",
      "        I-NP       0.90      0.98      0.94     29722\n",
      "        I-PP       0.92      0.93      0.93      4829\n",
      "       I-PRT       0.00      0.00      0.00       149\n",
      "      I-SBAR       0.00      0.00      0.00       366\n",
      "        I-VP       0.81      0.80      0.80      6802\n",
      "           O       0.93      0.98      0.95      6890\n",
      "\n",
      "    accuracy                           0.90     50921\n",
      "   macro avg       0.21      0.22      0.21     50921\n",
      "weighted avg       0.85      0.90      0.87     50921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_diag_pos[\"true_names\"], validation_diag_pos[\"pred_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       -PAD-       0.00      0.00      0.00         0\n",
      "      B-ADVP       0.00      0.00      0.00         5\n",
      "        B-NP       0.00      0.00      0.00       861\n",
      "        B-PP       0.00      0.00      0.00        43\n",
      "      B-SBAR       0.00      0.00      0.00         8\n",
      "        B-VP       0.00      0.00      0.00        40\n",
      "      I-ADJP       0.00      0.00      0.00       331\n",
      "      I-ADVP       0.00      0.00      0.00       585\n",
      "     I-CONJP       0.00      0.00      0.00        13\n",
      "      I-INTJ       0.00      0.00      0.00        13\n",
      "       I-LST       0.00      0.00      0.00        29\n",
      "        I-NP       0.91      0.98      0.94     28041\n",
      "        I-PP       0.92      0.92      0.92      3937\n",
      "       I-PRT       0.00      0.00      0.00       110\n",
      "      I-SBAR       0.00      0.00      0.00       292\n",
      "        I-VP       0.79      0.79      0.79      5629\n",
      "           O       0.94      0.97      0.95      6127\n",
      "\n",
      "    accuracy                           0.90     46064\n",
      "   macro avg       0.21      0.22      0.21     46064\n",
      "weighted avg       0.85      0.90      0.88     46064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_diag_pos[\"true_names\"], test_diag_pos[\"pred_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 128)           3024000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 50, 512)           788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 50, 9)             4617      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50, 9)             0         \n",
      "=================================================================\n",
      "Total params: 3,817,097\n",
      "Trainable params: 3,817,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ner_model = Sequential()\n",
    "ner_model.add(InputLayer(input_shape=(max_len, )))\n",
    "ner_model.add(Embedding(len(prepared_data_train[\"tag_dict\"][\"word_dict\"]), 128))\n",
    "ner_model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "ner_model.add(TimeDistributed(Dense(len(prepared_data_train[\"tag_dict\"][\"net_dict\"]))))\n",
    "ner_model.add(Activation('softmax'))\n",
    "\n",
    "ner_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(0.001),\n",
    "             metrics=['accuracy'])\n",
    "ner_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13095 samples, validate on 3034 samples\n",
      "Epoch 1/2\n",
      "13095/13095 [==============================] - 19s 1ms/step - loss: 0.3832 - accuracy: 0.9037 - val_loss: 0.2138 - val_accuracy: 0.9431\n",
      "Epoch 2/2\n",
      "13095/13095 [==============================] - 19s 1ms/step - loss: 0.1395 - accuracy: 0.9547 - val_loss: 0.1057 - val_accuracy: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13d4450f208>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model.fit(\n",
    "    prepared_data_train[\"x\"],\n",
    "    to_categorical(prepared_data_train[\"y_net\"], len(prepared_data_train[\"tag_dict\"][\"net_dict\"])),\n",
    "    batch_size=128, epochs=2,\n",
    "    validation_data = (\n",
    "        prepared_data_validation[\"x\"],\n",
    "        to_categorical(prepared_data_validation[\"y_net\"], len(prepared_data_train[\"tag_dict\"][\"net_dict\"]))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diag_net = remove_padding(predictions(ner_model,\n",
    "           prepared_data_train[\"x\"],\n",
    "           prepared_data_train[\"y_net\"],\n",
    "           inverse_dict=inversed_pos_dict))\n",
    "validation_diag_net = remove_padding(predictions(ner_model,\n",
    "           prepared_data_validation[\"x\"],\n",
    "           prepared_data_validation[\"y_net\"],\n",
    "           inverse_dict=inversed_pos_dict))\n",
    "test_diag_net= remove_padding(predictions(ner_model,\n",
    "           prepared_data_test[\"x\"],\n",
    "           prepared_data_test[\"y_net\"],\n",
    "           inverse_dict=inversed_net_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "F1 score: 0.886717\n",
      "Cohens kappa: 0.645111\n",
      "Accuracy: 0.901846\n",
      "Precision: 0.902489\n",
      "Recall: 0.901846\n",
      "Validation metrics:\n",
      "F1 score: 0.871354\n",
      "Cohens kappa: 0.577764\n",
      "Accuracy: 0.891263\n",
      "Precision: 0.880869\n",
      "Recall: 0.891263\n",
      "Test metrics:\n",
      "F1 score: 0.849952\n",
      "Cohens kappa: 0.507735\n",
      "Accuracy: 0.874392\n",
      "Precision: 0.860301\n",
      "Recall: 0.874392\n"
     ]
    }
   ],
   "source": [
    "print(\"Train metrics:\")\n",
    "print_diag(train_diag_net)\n",
    "print(\"Validation metrics:\")\n",
    "print_diag(validation_diag_net)\n",
    "print(\"Test metrics:\")\n",
    "print_diag(test_diag_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       -PAD-       0.00      0.00      0.00         0\n",
      "      I-ADJP       0.45      0.11      0.18      4552\n",
      "      I-ADVP       0.00      0.00      0.00        37\n",
      "     I-CONJP       0.97      1.00      0.98    169208\n",
      "       I-LST       0.00      0.00      0.00        24\n",
      "        I-NP       0.00      0.00      0.00        11\n",
      "      I-SBAR       0.83      0.14      0.25      9979\n",
      "        I-VP       0.52      0.54      0.53      8282\n",
      "           O       0.48      0.71      0.57     11077\n",
      "\n",
      "    accuracy                           0.90    203170\n",
      "   macro avg       0.36      0.28      0.28    203170\n",
      "weighted avg       0.90      0.90      0.89    203170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_diag_net[\"true_names\"], train_diag_net[\"pred_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       -PAD-       0.00      0.00      0.00         0\n",
      "      I-ADJP       0.42      0.09      0.14      1250\n",
      "      I-ADVP       0.00      0.00      0.00         4\n",
      "     I-CONJP       0.94      1.00      0.97     42452\n",
      "      I-SBAR       0.86      0.12      0.22      2090\n",
      "        I-VP       0.53      0.51      0.52      2084\n",
      "           O       0.49      0.54      0.51      3041\n",
      "\n",
      "    accuracy                           0.89     50921\n",
      "   macro avg       0.46      0.32      0.34     50921\n",
      "weighted avg       0.88      0.89      0.87     50921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_diag_net[\"true_names\"], validation_diag_net[\"pred_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       -PAD-       0.00      0.00      0.00         0\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-LOC       0.60      0.53      0.56      1900\n",
      "      I-MISC       0.49      0.10      0.17       905\n",
      "       I-ORG       0.81      0.12      0.22      2482\n",
      "       I-PER       0.38      0.36      0.37      2676\n",
      "           O       0.92      1.00      0.96     38081\n",
      "\n",
      "    accuracy                           0.87     46064\n",
      "   macro avg       0.36      0.23      0.25     46064\n",
      "weighted avg       0.86      0.87      0.85     46064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_diag_net[\"true_names\"], test_diag_net[\"pred_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHUNK TAG MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 128)           3024000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 50, 512)           788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 50, 46)            23598     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50, 46)            0         \n",
      "=================================================================\n",
      "Total params: 3,836,078\n",
      "Trainable params: 3,836,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "chunk_tag_model = Sequential()\n",
    "chunk_tag_model.add(InputLayer(input_shape=(max_len, )))\n",
    "chunk_tag_model.add(Embedding(len(prepared_data_train[\"tag_dict\"][\"word_dict\"]), 128))\n",
    "chunk_tag_model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "chunk_tag_model.add(TimeDistributed(Dense(len(prepared_data_train[\"tag_dict\"][\"chunk_dict\"]))))\n",
    "chunk_tag_model.add(Activation('softmax'))\n",
    "\n",
    "chunk_tag_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(0.001),\n",
    "             metrics=['accuracy'])\n",
    "chunk_tag_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13095 samples, validate on 3034 samples\n",
      "Epoch 1/2\n",
      "13095/13095 [==============================] - 18s 1ms/step - loss: 1.1657 - accuracy: 0.7195 - val_loss: 0.9327 - val_accuracy: 0.7344\n",
      "Epoch 2/2\n",
      "13095/13095 [==============================] - 18s 1ms/step - loss: 0.7181 - accuracy: 0.8111 - val_loss: 0.5574 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13d4479fc08>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_tag_model.fit(\n",
    "    prepared_data_train[\"x\"],\n",
    "    to_categorical(prepared_data_train[\"y_chunk\"], len(prepared_data_train[\"tag_dict\"][\"chunk_dict\"])),\n",
    "    batch_size=128, epochs=2,\n",
    "    validation_data = (\n",
    "        prepared_data_validation[\"x\"],\n",
    "        to_categorical(prepared_data_validation[\"y_chunk\"], len(prepared_data_train[\"tag_dict\"][\"chunk_dict\"]))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diag_chunk = remove_padding(predictions(ner_model,\n",
    "           prepared_data_train[\"x\"],\n",
    "           prepared_data_train[\"y_chunk\"],\n",
    "           inverse_dict=inversed_chunk_dict))\n",
    "validation_diag_chunk = remove_padding(predictions(ner_model,\n",
    "           prepared_data_validation[\"x\"],\n",
    "           prepared_data_validation[\"y_chunk\"],\n",
    "           inverse_dict=inversed_chunk_dict))\n",
    "test_diag_chunk= remove_padding(predictions(ner_model,\n",
    "           prepared_data_test[\"x\"],\n",
    "           prepared_data_test[\"y_chunk\"],\n",
    "           inverse_dict=inversed_chunk_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "F1 score: 0.000522\n",
      "Cohens kappa: -0.004949\n",
      "Accuracy: 0.001870\n",
      "Precision: 0.000423\n",
      "Recall: 0.001870\n",
      "Validation metrics:\n",
      "F1 score: 0.000482\n",
      "Cohens kappa: -0.004324\n",
      "Accuracy: 0.001983\n",
      "Precision: 0.000424\n",
      "Recall: 0.001983\n",
      "Test metrics:\n",
      "F1 score: 0.000404\n",
      "Cohens kappa: -0.003417\n",
      "Accuracy: 0.001650\n",
      "Precision: 0.000374\n",
      "Recall: 0.001650\n"
     ]
    }
   ],
   "source": [
    "print(\"Train metrics:\")\n",
    "print_diag(train_diag_chunk)\n",
    "print(\"Validation metrics:\")\n",
    "print_diag(validation_diag_chunk)\n",
    "print(\"Test metrics:\")\n",
    "print_diag(test_diag_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           \"       0.00      0.00      0.00      2171\n",
      "           $       0.00      0.00      0.00       427\n",
      "          ''       0.00      0.00      0.00        35\n",
      "           (       0.00      0.00      0.00      2864\n",
      "           )       0.00      0.00      0.00      2864\n",
      "           ,       0.00      0.00      0.00      7259\n",
      "       -PAD-       0.00      0.00      0.00         0\n",
      "           .       0.00      0.00      0.00      7389\n",
      "           :       0.00      0.00      0.00      2367\n",
      "          CC       0.00      0.00      0.00      3645\n",
      "          CD       0.00      0.00      0.00     19673\n",
      "          DT       0.00      0.00      0.00     13411\n",
      "          EX       0.00      0.00      0.00       136\n",
      "          FW       0.00      0.00      0.00       166\n",
      "          IN       0.00      0.00      0.00     19021\n",
      "          JJ       0.00      0.00      0.00     11800\n",
      "         JJR       0.00      0.00      0.00       381\n",
      "         JJS       0.00      0.96      0.00       253\n",
      "          LS       0.00      0.00      0.00        13\n",
      "          MD       0.00      0.00      0.00      1196\n",
      "          NN       0.00      0.00      0.00     23846\n",
      "         NNP       0.00      0.00      0.00     34320\n",
      "        NNPS       0.00      0.00      0.00       684\n",
      "         NNS       0.01      0.01      0.01      9886\n",
      "      NN|SYM       0.00      0.00      0.00         4\n",
      "         PDT       0.00      0.00      0.00        32\n",
      "         POS       0.00      0.00      0.00      1550\n",
      "         PRP       0.00      0.00      0.00      3154\n",
      "        PRP$       0.00      0.00      0.00      1515\n",
      "          RB       0.00      0.00      0.00      3963\n",
      "         RBR       0.00      0.00      0.00       162\n",
      "         RBS       0.00      0.00      0.00        35\n",
      "          RP       0.00      0.00      0.00       528\n",
      "         SYM       0.00      0.00      0.00       439\n",
      "          TO       0.00      0.00      0.00      3468\n",
      "          UH       0.00      0.00      0.00        30\n",
      "          VB       0.00      0.00      0.00      4245\n",
      "         VBD       0.00      0.00      0.00      8278\n",
      "         VBG       0.00      0.00      0.00      2579\n",
      "         VBN       0.00      0.00      0.00      4096\n",
      "         VBP       0.00      0.00      0.00      1432\n",
      "         VBZ       0.00      0.00      0.00      2419\n",
      "         WDT       0.00      0.00      0.00       504\n",
      "          WP       0.00      0.00      0.00       526\n",
      "         WP$       0.00      0.00      0.00        23\n",
      "         WRB       0.00      0.00      0.00       381\n",
      "\n",
      "    accuracy                           0.00    203170\n",
      "   macro avg       0.00      0.02      0.00    203170\n",
      "weighted avg       0.00      0.00      0.00    203170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_diag_chunk[\"true_names\"], train_diag_chunk[\"pred_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           \"       0.00      0.00      0.00       630\n",
      "           $       0.00      0.00      0.00       101\n",
      "          ''       0.00      0.00      0.00        11\n",
      "           (       0.00      0.00      0.00       665\n",
      "           )       0.00      0.00      0.00       669\n",
      "           ,       0.00      0.00      0.00      1917\n",
      "       -PAD-       0.00      0.00      0.00         0\n",
      "           .       0.00      0.00      0.00      1879\n",
      "           :       0.00      0.00      0.00       563\n",
      "          CC       0.00      0.00      0.00       932\n",
      "          CD       0.00      0.00      0.00      4239\n",
      "          DT       0.00      0.00      0.00      3502\n",
      "          EX       0.00      0.00      0.00        39\n",
      "          FW       0.00      0.00      0.00        26\n",
      "          IN       0.00      0.00      0.00      4958\n",
      "          JJ       0.00      0.00      0.00      3030\n",
      "         JJR       0.00      0.00      0.00       105\n",
      "         JJS       0.00      0.95      0.00        77\n",
      "          LS       0.00      0.00      0.00         1\n",
      "          MD       0.00      0.00      0.00       298\n",
      "          NN       0.00      0.00      0.00      6008\n",
      "         NNP       0.00      0.00      0.00      8433\n",
      "        NNPS       0.00      0.00      0.00       163\n",
      "         NNS       0.01      0.01      0.01      2492\n",
      "      NN|SYM       0.00      0.00      0.00         1\n",
      "         PDT       0.00      0.00      0.00         7\n",
      "         POS       0.00      0.00      0.00       421\n",
      "         PRP       0.00      0.00      0.00       855\n",
      "        PRP$       0.00      0.00      0.00       421\n",
      "          RB       0.00      0.00      0.00       983\n",
      "         RBR       0.00      0.00      0.00        53\n",
      "         RBS       0.00      0.00      0.00        18\n",
      "          RP       0.00      0.00      0.00       150\n",
      "         SYM       0.00      0.00      0.00        86\n",
      "          TO       0.00      0.00      0.00       905\n",
      "          UH       0.00      0.00      0.00         5\n",
      "          VB       0.00      0.00      0.00      1115\n",
      "         VBD       0.00      0.00      0.00      2223\n",
      "         VBG       0.00      0.00      0.00       699\n",
      "         VBN       0.00      0.00      0.00       991\n",
      "         VBP       0.00      0.00      0.00       359\n",
      "         VBZ       0.00      0.00      0.00       508\n",
      "         WDT       0.00      0.00      0.00       155\n",
      "          WP       0.00      0.00      0.00       127\n",
      "         WP$       0.00      0.00      0.00         9\n",
      "         WRB       0.00      0.00      0.00        92\n",
      "\n",
      "    accuracy                           0.00     50921\n",
      "   macro avg       0.00      0.02      0.00     50921\n",
      "weighted avg       0.00      0.00      0.00     50921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_diag_chunk[\"true_names\"], validation_diag_chunk[\"pred_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           \"       0.00      0.00      0.00       419\n",
      "           $       0.00      0.00      0.00        94\n",
      "          ''       0.00      0.00      0.00        14\n",
      "           (       0.00      0.00      0.00       675\n",
      "           )       0.00      0.00      0.00       674\n",
      "           ,       0.00      0.00      0.00      1604\n",
      "       -PAD-       0.00      0.00      0.00         0\n",
      "           .       0.00      0.00      0.00      1629\n",
      "           :       0.00      0.00      0.00       545\n",
      "          CC       0.00      0.00      0.00       762\n",
      "          CD       0.00      0.00      0.00      5922\n",
      "          DT       0.00      0.00      0.00      2786\n",
      "          EX       0.00      0.00      0.00        34\n",
      "          FW       0.00      0.00      0.00        33\n",
      "          IN       0.00      0.00      0.00      4005\n",
      "          JJ       0.00      0.00      0.00      2383\n",
      "         JJR       0.00      0.00      0.00        91\n",
      "         JJS       0.00      1.00      0.00        56\n",
      "          LS       0.00      0.00      0.00        23\n",
      "          MD       0.00      0.00      0.00       268\n",
      "          NN       0.00      0.00      0.00      4915\n",
      "         NNP       0.00      0.00      0.00      8468\n",
      "        NNPS       0.00      0.00      0.00       159\n",
      "         NNS       0.01      0.01      0.01      2165\n",
      "         PDT       0.00      0.00      0.00         7\n",
      "         POS       0.00      0.00      0.00       347\n",
      "         PRP       0.00      0.00      0.00       603\n",
      "        PRP$       0.00      0.00      0.00       294\n",
      "          RB       0.00      0.00      0.00       884\n",
      "         RBR       0.00      0.00      0.00        42\n",
      "         RBS       0.00      0.00      0.00         9\n",
      "          RP       0.00      0.00      0.00       106\n",
      "         SYM       0.00      0.00      0.00       117\n",
      "          TO       0.00      0.00      0.00       817\n",
      "          UH       0.00      0.00      0.00         7\n",
      "          VB       0.00      0.00      0.00       931\n",
      "         VBD       0.00      0.00      0.00      1695\n",
      "         VBG       0.00      0.00      0.00       483\n",
      "         VBN       0.00      0.00      0.00       864\n",
      "         VBP       0.00      0.00      0.00       329\n",
      "         VBZ       0.00      0.00      0.00       501\n",
      "         WDT       0.00      0.00      0.00       108\n",
      "          WP       0.00      0.00      0.00       113\n",
      "         WP$       0.00      0.00      0.00         9\n",
      "         WRB       0.00      0.00      0.00        74\n",
      "\n",
      "    accuracy                           0.00     46064\n",
      "   macro avg       0.00      0.02      0.00     46064\n",
      "weighted avg       0.00      0.00      0.00     46064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_diag_chunk[\"true_names\"], test_diag_chunk[\"pred_names\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
